{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d74ee8f-2dd4-408d-a2df-1c47230a08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29137d97-d3a0-46af-900e-e97c708d0cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import re\n",
    "import transformers\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2d66f5-fa07-4009-81c2-299e8ca3d5b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 處理data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1104d4-d59a-4b75-94be-357f62a9b656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('prepare_data/pattern4.json', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b54c3c57-186d-4412-8fd6-a7fac42d6c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_data = {}\n",
    "verb_data = {}\n",
    "for i in data:\n",
    "    data_list = []\n",
    "    verb_list = []\n",
    "    for j in data[i]:\n",
    "        text = j['example'].replace('<span class=\"x\">','').replace('<span class=\"cl\">','').replace('</span>','').strip()\n",
    "        new_text = ''\n",
    "        test = 0\n",
    "        for k in text.split(' '):\n",
    "            new_text += ' '\n",
    "            new_text += k\n",
    "            if i == k:\n",
    "                test += 1\n",
    "                break\n",
    "        if test != 0:\n",
    "            data_list.append({'pattern': j['pattern'], 'text': new_text.strip()})\n",
    "        if test == 0:\n",
    "            verb_list.append({'pattern': j['pattern'], 'text': new_text.strip()})\n",
    "    if data_list != []:\n",
    "        new_data[i] = data_list\n",
    "    if verb_list != []:\n",
    "        verb_data[i] = verb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa01ea44-1265-49cd-8aeb-1835f829b7b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'pattern': 'abandon something', 'text': 'Snow forced many drivers to abandon'}, {'pattern': 'abandon something', 'text': 'Snow forced many drivers to abandon'}, {'pattern': 'abandon something to somebody/something', 'text': 'They had to abandon'}]\n",
      "[{'pattern': 'abandon somebody', 'text': 'The baby had been abandoned by its mother.'}, {'pattern': 'abandon somebody', 'text': 'The baby had been abandoned by its mother.'}, {'pattern': 'abandon somebody to something', 'text': '‘We have been abandoned to our fate,’ said one resident.'}, {'pattern': 'abandon something', 'text': 'They abandoned the match because of rain.'}, {'pattern': 'abandon somebody', 'text': 'The country abandoned its political leaders after the war.'}, {'pattern': 'abandon somebody', 'text': 'The country abandoned its political leaders after the war.'}, {'pattern': 'abandon something', 'text': 'By 1930 he had abandoned his Marxist principles.'}]\n"
     ]
    }
   ],
   "source": [
    "print(new_data['abandon'])\n",
    "print(verb_data['abandon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f307177-6974-4f9c-a689-320cc549c865",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3663"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(verb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6454c9ea-2750-4994-a574-af92c6c9b3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(new_data, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c55fa1f-af43-4523-8be7-739f62ebf46c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 改成dataset形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d8574a9-46e3-4168-b361-83a59b4863b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data.json', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d782d54-5849-49e5-a31b-ef767aae00e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with jsonlines.open('data.jsonlines','w') as f:\n",
    "    for i in data:\n",
    "        for j in data[i]:\n",
    "            f.write(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c066f00c-3f55-4c68-90ff-6996d493d14b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"data.jsonlines\",\"r\") as f:\n",
    "    temp = set(f.readlines())\n",
    "with open(\"data1.jsonlines\",\"w\") as w:\n",
    "    for i in temp:\n",
    "        w.write(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e8501-e616-41ef-8e25-4d4fbac777a0",
   "metadata": {},
   "source": [
    "### import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fc5a086-4766-4f57-a874-4a273ba08341",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"json\", data_files=\"verb1.jsonlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "602b2f20-a563-4dcc-85c9-20dd18a56346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pattern', 'text'],\n",
       "        num_rows: 17352\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d28a5-9a8d-432a-be1e-edcb264badd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b81e8d4d-4a23-4fec-9629-37334b725f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_train_test = datasets[\"train\"].train_test_split(test_size=1500)\n",
    "\n",
    "datasets[\"train\"] = datasets_train_test[\"train\"]\n",
    "datasets[\"validation\"] = datasets_train_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c15c9e-6b15-4d56-bc0d-f0f91ffc28a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pattern', 'text'],\n",
       "        num_rows: 15852\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pattern', 'text'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bf336ed-deb8-4ac0-be17-136ccb1d33ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/nlplab/maggie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb5b0c05-1396-4b0a-9bbc-271b6afb2543",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "max_input_length = 256\n",
    "max_target_length = 64\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inputs = [prefix + text for text in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"pattern\"], max_length=max_target_length, \n",
    "                       truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d6052e9-047d-4768-9c92-04fdbbd47e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a35b6490cc474e6eb6cb758d783ba982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15852 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f972bf6ae104f90b2c43f91270a3351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e92114e3-ac06-47db-8221-97bd3f665d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pattern', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15852\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['pattern', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5451ba92-5917-42af-9c0b-aa4372f79f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94af0a3e-1ed5-4bf5-86c3-6e04d45b8daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "model_name = \"t5-base-medium-title-generation\"\n",
    "model_dir = f\"t5_small\"\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    model_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eae64d2e-bdcc-4dca-ac62-4c707fa54512",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afe0b82d-79ca-4a3e-8cc7-7be1601527a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-7154407f1f07>:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n",
      "/home/nlplab/maggie/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a36b7468-74cb-406c-be9b-24044b072de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip()))\n",
    "                      for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) \n",
    "                      for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "\n",
    "    # Extract ROUGE f1 scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length to metrics\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id)\n",
    "                      for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "362a13b1-28b3-4fbc-b4f3-50c68cb38f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Function that returns an untrained model to be trained\n",
    "def model_init():\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b96e2bfd-dbd7-48c5-9eba-bdfc5cf6fd86",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1982' max='1982' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1982/1982 09:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.846000</td>\n",
       "      <td>3.034602</td>\n",
       "      <td>31.862900</td>\n",
       "      <td>8.596300</td>\n",
       "      <td>31.610400</td>\n",
       "      <td>31.591000</td>\n",
       "      <td>8.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.875800</td>\n",
       "      <td>2.093965</td>\n",
       "      <td>50.019600</td>\n",
       "      <td>23.721800</td>\n",
       "      <td>49.790100</td>\n",
       "      <td>49.763400</td>\n",
       "      <td>5.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.212600</td>\n",
       "      <td>1.745479</td>\n",
       "      <td>54.698100</td>\n",
       "      <td>28.362900</td>\n",
       "      <td>54.478900</td>\n",
       "      <td>54.452000</td>\n",
       "      <td>4.520700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.889500</td>\n",
       "      <td>1.542307</td>\n",
       "      <td>58.330000</td>\n",
       "      <td>32.144800</td>\n",
       "      <td>57.947900</td>\n",
       "      <td>57.970000</td>\n",
       "      <td>4.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.673500</td>\n",
       "      <td>1.438499</td>\n",
       "      <td>60.318600</td>\n",
       "      <td>34.328200</td>\n",
       "      <td>59.997400</td>\n",
       "      <td>59.994900</td>\n",
       "      <td>4.344700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.628100</td>\n",
       "      <td>1.352460</td>\n",
       "      <td>61.263100</td>\n",
       "      <td>34.997600</td>\n",
       "      <td>60.963100</td>\n",
       "      <td>60.992000</td>\n",
       "      <td>4.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.503700</td>\n",
       "      <td>1.288601</td>\n",
       "      <td>62.490400</td>\n",
       "      <td>36.817000</td>\n",
       "      <td>62.182500</td>\n",
       "      <td>62.199300</td>\n",
       "      <td>4.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.513400</td>\n",
       "      <td>1.238064</td>\n",
       "      <td>63.661400</td>\n",
       "      <td>38.349100</td>\n",
       "      <td>63.310000</td>\n",
       "      <td>63.361000</td>\n",
       "      <td>4.449300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.454000</td>\n",
       "      <td>1.217188</td>\n",
       "      <td>63.528100</td>\n",
       "      <td>38.413700</td>\n",
       "      <td>63.236300</td>\n",
       "      <td>63.280000</td>\n",
       "      <td>4.321300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.431900</td>\n",
       "      <td>1.181065</td>\n",
       "      <td>63.971700</td>\n",
       "      <td>38.578300</td>\n",
       "      <td>63.632400</td>\n",
       "      <td>63.673600</td>\n",
       "      <td>4.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.389800</td>\n",
       "      <td>1.150562</td>\n",
       "      <td>64.630400</td>\n",
       "      <td>39.575900</td>\n",
       "      <td>64.311700</td>\n",
       "      <td>64.343300</td>\n",
       "      <td>4.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.331600</td>\n",
       "      <td>1.132176</td>\n",
       "      <td>65.057100</td>\n",
       "      <td>40.346300</td>\n",
       "      <td>64.777700</td>\n",
       "      <td>64.808000</td>\n",
       "      <td>4.428700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.291600</td>\n",
       "      <td>1.120913</td>\n",
       "      <td>64.804500</td>\n",
       "      <td>40.170000</td>\n",
       "      <td>64.493200</td>\n",
       "      <td>64.530000</td>\n",
       "      <td>4.504000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.370300</td>\n",
       "      <td>1.100694</td>\n",
       "      <td>65.040700</td>\n",
       "      <td>40.293400</td>\n",
       "      <td>64.760600</td>\n",
       "      <td>64.774300</td>\n",
       "      <td>4.456700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.301800</td>\n",
       "      <td>1.090915</td>\n",
       "      <td>65.226900</td>\n",
       "      <td>41.170900</td>\n",
       "      <td>64.899800</td>\n",
       "      <td>64.945900</td>\n",
       "      <td>4.554700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.249700</td>\n",
       "      <td>1.086477</td>\n",
       "      <td>65.539300</td>\n",
       "      <td>41.709100</td>\n",
       "      <td>65.263300</td>\n",
       "      <td>65.270600</td>\n",
       "      <td>4.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.265700</td>\n",
       "      <td>1.077625</td>\n",
       "      <td>65.647100</td>\n",
       "      <td>41.648500</td>\n",
       "      <td>65.366800</td>\n",
       "      <td>65.378800</td>\n",
       "      <td>4.466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.264500</td>\n",
       "      <td>1.071435</td>\n",
       "      <td>65.658800</td>\n",
       "      <td>41.781600</td>\n",
       "      <td>65.363500</td>\n",
       "      <td>65.381600</td>\n",
       "      <td>4.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.243200</td>\n",
       "      <td>1.069686</td>\n",
       "      <td>65.651300</td>\n",
       "      <td>41.917900</td>\n",
       "      <td>65.360800</td>\n",
       "      <td>65.399200</td>\n",
       "      <td>4.472000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1982, training_loss=1.7015805364737717, metrics={'train_runtime': 552.8646, 'train_samples_per_second': 28.672, 'train_steps_per_second': 3.585, 'total_flos': 74163077775360.0, 'train_loss': 1.7015805364737717, 'epoch': 1.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8c3ebb9-c5d6-4db4-b08a-89195786fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('gen4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c155cccb-c905-4721-9b8f-697536357430",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faf4cdff-42bb-4cf1-842b-fab50e5b0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"t5-base-medium-title-generation/checkpoint-2000\"\n",
    "model_dir = f\"gen4\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "\n",
    "max_input_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d6e5e2d-4439-4947-ae3a-34ce0772a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(text):\n",
    "    inputs = [\"summarize: \" + text]\n",
    "    inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
    "    output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=2, max_length=128)\n",
    "    decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "    predicted = nltk.sent_tokenize(decoded_output.strip())[0]\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08888cad-6549-4015-b21d-3cf727e207f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'available to somebody/something'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Not all the facts are made available\"\n",
    "\n",
    "gen(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e528ab0-3907-4762-9f58-9ec023f4632f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
